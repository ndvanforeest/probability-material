\documentclass[tf-tutorial-all.tex]{subfiles}
\begin{document}

% \setcounter{section}{0}
% \section{TF questions PD Week 1}


\begin{truefalse}
$X$ is a rv, $X  \in \R$. Claim: $\supp{X} = (c, \infty) \implies \P{X\leq c} = 0$.
\begin{solution}
Yes.
\end{solution}
\end{truefalse}

\begin{truefalse}
Suppose $X$ a real-valued rv with $\supp X = [0, c]$. Claim:
\begin{equation}
\V X = \E{X^2} - (\E{X})^2 \leq c \E X - (\E X)^2 = (c-\E X)\E X.
\end{equation}
\begin{solution}
Yes.
\end{solution}
\end{truefalse}


\begin{truefalse}
Let $X$ and $Y$ be independent rvs. Claim: $F_{X+Y}(x, y) = F_X(x) + F_{Y}(y)$.
\begin{solution}
False. We should write $F_{X,Y}$ rather than $F_{X+Y}$ , and since the rvs are independent, consider the product of the CDFs, not the sum.
\end{solution}
\end{truefalse}

\begin{truefalse}
Let $M_{X}(s)$ be the moment generating function of some rv $X$.  Claim:
\begin{equation}
M_{X}(0) = 0.
\end{equation}
\begin{solution}
It's false.
\end{solution}
\end{truefalse}


\begin{truefalse}
Let $M_{X}(s)$ be the moment generating function of some rv $X$.  Claim:
\begin{equation}
\left(\frac{\d}{\d s}\right)^{2 }M_{X}(s)|_{s=0} =  \V X + (\E{X})^{2}.
\end{equation}
\begin{solution}
It's true.
\end{solution}
\end{truefalse}


\begin{truefalse}
We have two positive rvs $X$ and $Y$. Claim: $\V{X+Y} = \V X + \V Y$.
\begin{solution}
It's false, it's not given that $X$  and $Y$ are independent.
\end{solution}
\end{truefalse}



\begin{truefalse}
We have two independent positive rvs $X$ and $Y$. Claim: $M_{2X+Y}(s)  = (M_{X}(s))^{2} M_{Y}(s)$.
\begin{solution}
It's false in general, because $X$ is not independent of itself.
\end{solution}
\end{truefalse}




\begin{truefalse}
People enter a shop such that the time $X$ between  any two consecutive customers is $X\sim \Exp{\lambda}$ with $\lambda=10$ per h. Claim: $\P{X > x} = e^{-\lambda x}$, for $x\geq 0$.
\begin{solution}
It's correct, see BH.5.45.
\end{solution}
\end{truefalse}

\begin{truefalse}
People enter a shop such that the time $X$ between any two consecutive customers is $X\sim \Exp{\lambda}$ with $\lambda=10$ per h.
Assume that the interarrival times between customers are iid.
Let $N(t)$ be the number of people that enter during a time interval of length $t$.
Claim $N(t) \sim \Pois{\lambda}$.
\begin{solution}
It's false, see BH.5.45. It's $\sim \Pois{\lambda t}$.
\end{solution}
\end{truefalse}

\begin{truefalse}
People enter a shop such that the time $X$ between any two consecutive customers is $X\sim \Exp{\lambda}$ with $\lambda=10$ per h.
Assume that the interarrival times between customers are iid.
Let $N(t)$ be the number of people that enter during a time interval of length $t$.
Suppose that $T_{3}$ is the time the third person enters.
Claim: $\P{N(t) <3} = \P{T_{3}>t}$.
\begin{solution}
It's true.
\end{solution}
\end{truefalse}





\begin{truefalse}
Write $m$ for the median $\E X$ of the rv $X$. Claim, the following definition correct:
\begin{equation*}
\V X := \E{X^2} - m^{2}.
\end{equation*}
\begin{solution}
False, $\E X$ need not be equal to the median, moreover the \emph{definition} of the variance involves the mean, not the median.
\end{solution}
\end{truefalse}


\begin{truefalse}
    For two events $A$ and $B$. Claim: $\E{\1{A}\1{B}} = P(A) + P(B) - P(A \cup B)$
\begin{solution}
It's True
\end{solution}
\end{truefalse}

\begin{truefalse}
For an unfair 4-sided die that throws 4 half of the time and 1 to 3 with equal probability.
If the rv
$X$ denotes the thrown value of the dice.
Claim: $\E{X^2} = \frac{38}{3}$
\begin{solution}
It's False. Calculating using LOTUS gives $\E{X^2} = \frac{1}{2} 4^{2} + \frac{1}{2\cdot3}(1 + 4 + 9) = \frac{31}{3}$
\end{solution}
\end{truefalse}

\begin{truefalse}
For a degenerate rv $X$ and $c$ an arbitrary, non-zero constant. Claim: $\V{cX} > 0$
\begin{solution}
It's False. the variance of a degenerate rv is always 0.
\end{solution}
\end{truefalse}

\begin{truefalse}
Assume that $\V{X} = \sigma^2$ and $\E{X^2} = a^2$ both exist and are finite. Claim:$ \E{X} = \sqrt{a^2-\sigma^2}$
\begin{solution}
It's True.
\end{solution}
\end{truefalse}

\begin{truefalse}
Given $\V{X+Y} = \V{X} + \V{Y}$. Claim: $X$ and $Y$ are independent,
\begin{solution}
It's False. Independence is sufficient but not necessary for the equality to hold.
\end{solution}
\end{truefalse}

\begin{truefalse}
For two rvs $X$ and $Y$, where $Y$ is always equal to $X$, given $\V{X}>0$. Claim: $\V{X+Y} = \V{X} + \V{Y}$
\begin{solution}
It's False, since if $Y$ is always equal to $X$ they are definitely not independent.
(see BH.
p.
172) In fact, as $Y=X$, $\V{X+Y} = \V{2X} = 4 \V X$.
Isn't it a bit counter intuitive that when $X$ and $Y$ are dependent like this, the variance is larger than if they would be independent?
\end{solution}
\end{truefalse}

%5.1 Q1
\begin{truefalse}
The expectation of a continuous random variable must always be nonnegative given that the probabilty density function values are nonnegative. (i.e. , $f(x) \geq 0$,)
\begin{solution}
False.\\
The density values are nonnegative. The values that the RV can attain can be negative, therefore the expectation may be negative.
\end{solution}
\end{truefalse}

%5.1 Q2
\begin{truefalse}
Let $X \sim \Norm{0, 1}$. We then know that $\P{X = 0} > \P{X = 5}$.
\begin{solution}
False. By definition $\P(X = k) = 0 \quad \forall k $.
\end{solution}
\end{truefalse}

%5.2 Q1
\begin{truefalse}
Let $U \sim \Unif{(a,b)}$ and  $(c,d) \subset (a,b)$. Then the conditional distribution of $U$ given that $U \in (c,d)$ is $\Unif{(c,d)}$.
\begin{solution}
True.
\\See Blitzstein proposition 5.2.3:\\
\textit{Proof.} For $u$ in $(c, d)$, the conditional CDF at $u$ is
$$
P(U \leq u \mid U \in(c, d))=\frac{P(U \leq u, c<U<d)}{P(U \in(c, d))}=\frac{P(U \in(c, u])}{P(U \in(c, d))}=\frac{u-c}{d-c} .
$$
The conditional CDF is 0 for $u \leq c$ and 1 for $u \geq d$. So the conditional distribution of $U$ is as claimed.
$\square$
\end{solution}
\end{truefalse}

%5.2 Q2
\begin{truefalse}
Let $U \sim \Unif{(a,b)}$. The distribution of $c^{2} \log(d) U + e - f^{4}$ is still uniform when $c,d, e $ and $ f \in R$. \\
% you can say the constants to be in N as a version. Or apply a non linear transformation.
\begin{solution}
True.\\
This still is a linear transformation. Let $c^{2} \log(d)$ be a constant $c_{1}$. And $e - f^{4}$ be a constant $c_{2}$. Then the question becomes $c_{1} U + c_{2} $, this still is linear by Blitzstein 5.2.6.
\end{solution}
\end{truefalse}

%5.4 Q1
\begin{truefalse}
Let $Z \sim \Norm{0,1}$ then $\Phi(z) = \Phi(-z)$ due to symmetry.
\begin{solution}
False.\\
This equation for symmetry does not hold for the CDF. \\
We have $\phi(z) = \phi(-z)$
\\And $\Phi(z) = 1 - \Phi(-z)$
\end{solution}
\end{truefalse}

%5.4 Q2
\begin{truefalse}
Let $Z \sim \Norm{\mu, \sigma^{2}}$. Let $X = Z \cdot \sigma^{-1} - \mu \cdot \sigma^{-1}$ then $X \sim \Norm{0,1}$.
\begin{solution}
True.
\\Rewriting results in $X = \frac{Z - \mu}{\sigma} $ Then $ X \sim \Norm{0,1}$ by definition.
\end{solution}
\end{truefalse}

%5.5 Q1
\begin{truefalse}
Let $X \sim \Exp{1}$ and $Y = \lambda X$, then $Y \sim \Exp{\lambda}$.
\begin{solution}
False. It should become $Y \sim \Exp{\frac{1}{\lambda}}$.
\end{solution}
\end{truefalse}

%5.5 Q2
\begin{truefalse}
Let $X \sim \Exp{\lambda}$, then:
$$ \int^{\infty}_{0} x  \lambda e^{ - \lambda x} \d x  =  \int^{\infty}_{- \infty} x \lambda e^{ - \lambda x} \1{x> 0}\d x.$$
\begin{solution}
True.\\
\end{solution}
\end{truefalse}

%5.5 Q3
\begin{truefalse}
For the Exponential distribution holds that:
$$\P{X > t + s | X > t} = \P{X > t}$$
\begin{solution}
 False.
 \\It should be :
$\P{X > t + s | X > s} = \P{X > t}$
\\This is the memoryless property.
% A version could be the correct answer, then the solution would be true.
\end{solution}
\end{truefalse}

%5.6 Q1
\begin{truefalse}
Let there be three cars in three different painting workshops, that are painted independently, and at the same time. The paint times in hours are iid and follow $X\sim \Exp{\lambda}$, with $\E X = 1/ \lambda = 3$.
Claim: The expected time for the first two cars to be finished is 2.5 hours.

\begin{solution}
True.
Due to independence and memorylessness we have that the expected time for the first two cars to finish is $ T = T_{1} + T_{2}$, as we only need the first two painting jobs;
$T_{1}$ is the time for the first job to finish, $T_{2}$ is the additional time for the second job to finish.
Clearly $T_{1} = \min\{X_{1}, X_{2}, X_{3}\}$,  and, after a restart (recall, memoryless), $T_{2} = \min\{X_{1}, X_{2}\}$. Hence,
\begin{equation*}
\E T = \frac{1}{3 \lambda} + \frac{1}{2\lambda} = 1 + 1.5 = 2.5.
\end{equation*}
For more reference, see 5.6.3 and 5.6.5 in Blitzstein.
\end{solution}
\end{truefalse}



\begin{truefalse}
Let $A$, $B$ be two arbitrary events. Claim: $\P{A|B}=\frac{\P{A \cap B}}{\P{B}}$.
\begin{solution}
False. The condition $\P{B}>0$ is missing.
\end{solution}
\end{truefalse}





\begin{truefalse}
Let $A$, $B$ be events s.t. $\P{A},\P{B}>0$. Claim: $\P{A|B}\P{B}=\P{B|A}\P{A}$.
\begin{solution}
True.
\end{solution}
\end{truefalse}



\begin{truefalse}
Let $A$, $B$, $C$ be events s.t. $\P{A \cap B}>0$. Claim:
\begin{equation}
\frac{\P{B \cap C | A}}{\P{B|A}}=\frac{\P{A \cap C|B}}{\P{A|B}}
\end{equation}
\begin{solution}
True. Both sides are equal to $\P{C|A,B}$.
\end{solution}
\end{truefalse}





\begin{truefalse}
Let $A$, $B$, $C$ be events s.t. $\P{C}>0$. Claim:
\begin{equation}
\P{A \cap B | C} + \P{A \cup B | C}=\P{A|C}+\P{B|C}
\end{equation}
\begin{solution}
True.
\end{solution}
\end{truefalse}





\begin{truefalse}
Let $A$ and $B$ be two disjoint events. Claim: $A$ and $B$ are dependent.
\begin{solution}
False. If $A$ and $B$ are disjoint, they can be only independent if  $\P{A}=0$ or $\P{B}=0$.
\end{solution}
\end{truefalse}


\begin{truefalse}
Suppose $A_i$ for $i=1, 2, \dots, n$ are independent indicator random
variables. Then $\sum_{i=1}^n A_i$ has a Binomial distribution.
\begin{solution}
False. They also need to have identical distributions.
\end{solution}
\end{truefalse}

\begin{truefalse}
If $X$ and $Y$ are independent, then for functions $g$ and
$f$: $g(X)$ is independent of $f(Y)$.
\begin{solution}
True.
\end{solution}
\end{truefalse}

\begin{truefalse}
A discrete random variable has a finite number of outcomes.
\begin{solution}
False. It has a countable number of outcomes, but it may be infinite, e.g. $\N$.
\end{solution}
\end{truefalse}

\begin{truefalse}
Let $X \sim \HGeom{3, 5, 2}$. Claim: $\P{X = 3} = 0$.
\begin{solution}
True. $3$ is not in the support of X: $\{0, 1, 2\}$.
\end{solution}
\end{truefalse}

\end{document}
