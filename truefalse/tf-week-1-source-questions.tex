\documentclass[tf-tutorial-all.tex]{subfiles}
\begin{document}

% \setcounter{section}{0}
% \section{TF questions PD Week 1}


\begin{truefalse}
$X$ is a rv, $X  \in \R$. Claim: $\supp{X} = (c, \infty) \implies \P{X\leq c} = 0$.
\begin{solution}
Yes.
\end{solution}
\end{truefalse}

\begin{truefalse}
Suppose $X$ is a real-valued rv with $\supp X = [0, c]$. Claim:
\begin{equation}
\V X = \E{X^2} - (\E{X})^2 \leq c \E X - (\E X)^2 = (c-\E X)\E X.
\end{equation}
\begin{solution}
Yes.
\end{solution}
\end{truefalse}


\begin{truefalse}
Let $X$ and $Y$ be independent rvs. Claim: $F_{X+Y}(x, y) = F_X(x) + F_{Y}(y)$.
\begin{solution}
False. We should write $F_{X,Y}$ rather than $F_{X+Y}$ , and since the rvs are independent, consider the product of the CDFs, not the sum.
\end{solution}
\end{truefalse}

\begin{truefalse}
Let $M_{X}(s)$ be the moment generating function of some rv $X$.  Claim:
\begin{equation}
M_{X}(0) = 0.
\end{equation}
\begin{solution}
It's false. Recall the definition: $\E{e^{0\cdot X}} = \E{e^0} = 1$.
\end{solution}
\end{truefalse}


\begin{truefalse}
Let $M_{X}(s)$ be the moment generating function of some rv $X$.  Claim:
\begin{equation}
\left(\frac{\d}{\d s}\right)^{2 }M_{X}(s)|_{s=0} =  \V X + (\E{X})^{2}.
\end{equation}
\begin{solution}
It's true.
\end{solution}
\end{truefalse}


\begin{truefalse}
We have two positive rvs $X$ and $Y$. Claim: $\V{X+Y} = \V X + \V Y$.
\begin{solution}
It's false, it's not given that $X$  and $Y$ are independent.
\end{solution}
\end{truefalse}



\begin{truefalse}
We have two independent positive rvs $X$ and $Y$. Claim: $M_{2X+Y}(s)  = (M_{X}(s))^{2} M_{Y}(s)$.
\begin{solution}
It's false in general, because $X$ is not independent of itself.
\end{solution}
\end{truefalse}




\begin{truefalse}
People enter a shop such that the time $X$ between  any two consecutive customers is $X\sim \Exp{\lambda}$ with $\lambda=10$ per hour. Claim: $\P{X > x} = e^{-\lambda x}$, for $x\geq 0$.
\begin{solution}
It's correct, see BH.5.45.
\end{solution}
\end{truefalse}

\begin{truefalse}
People enter a shop such that the time $X$ between any two consecutive customers is $X\sim \Exp{\lambda}$ with $\lambda=10$ per hour.
Assume that the interarrival times between customers are iid.
Let $N(t)$ be the number of people that enter during an interval $[0,t]$.
Claim $N(t) \sim \Pois{\lambda}$.
\begin{solution}
It's false, see BH.5.45. It's $\sim \Pois{\lambda t}$.
\end{solution}
\end{truefalse}

\begin{truefalse}
People enter a shop such that the time $X$ between any two consecutive customers is $X\sim \Exp{\lambda}$ with $\lambda=10$ per hour.
Assume that the interarrival times between customers are iid.
Let $N(t)$ be the number of people that enter during an interval $[0,t]$.
Suppose that $T_{3}$ is the time the third person enters.
Claim: $\P{N(t) <3} = \P{T_{3}>t}$.
\begin{solution}
It's true.
\end{solution}
\end{truefalse}




\begin{truefalse}
Write $m$ for the median of the rv $X$. Claim: the following definition is correct:
\begin{equation*}
\V X := \E{X^2} - m^{2}.
\end{equation*}
\begin{solution}
False, $\E X$ need not be equal to the median $m$, and  the \emph{definition} of the variance involves the mean, not the median.
\end{solution}
\end{truefalse}


\begin{truefalse}
    For two events $A$ and $B$. Claim: $\E{\1{A}\1{B}} = \P{A} + \P{B} - \P{A \cup B}$
\begin{solution}
It's True
\end{solution}
\end{truefalse}

\begin{truefalse}
For an unfair 4-sided die that throws 4 half of the time and 1 to 3 with equal probability.
If the rv
$X$ denotes the thrown value of the dice.
Claim: $\E{X^2} = \frac{38}{3}$
\begin{solution}
It's False. Calculating using LOTUS gives $\E{X^2} = \frac{1}{2} 4^{2} + \frac{1}{2\cdot3}(1 + 4 + 9) = \frac{31}{3}$
\end{solution}
\end{truefalse}

\begin{truefalse}
For a degenerate rv $X$ and $c$ an arbitrary, non-zero constant. Claim: $\V{cX} > 0$
\begin{solution}
It's False. the variance of a degenerate rv is always 0. See the warning in BH.4.1.3 in which degeneracy is discussed.
\end{solution}
\end{truefalse}

\begin{truefalse}
Assume that $\V{X} = \sigma^2$ and $\E{X^2} = a^2$ both exist and are finite. Claim:$ \E{X} = \sqrt{a^2-\sigma^2}$
\begin{solution}
It's True.
\end{solution}
\end{truefalse}

\begin{truefalse}
Given $\V{X+Y} = \V{X} + \V{Y}$. Claim: $X$ and $Y$ are independent,
\begin{solution}
It's False. Independence is sufficient but not necessary for the equality to hold.
\end{solution}
\end{truefalse}

\begin{truefalse}
For two rvs $X$ and $Y$, where $Y$ is always equal to $X$, given $\V{X}>0$. Claim: $\V{X+Y} = \V{X} + \V{Y}$
\begin{solution}
It's False, since if $Y$ is always equal to $X$ they are definitely not independent.
(see BH. p. 172) In fact, as $Y=X$, $\V{X+Y} = \V{2X} = 4 \V X$.
Isn't it a bit counter intuitive that when $X$ and $Y$ are dependent like this, the variance is larger than if they would be independent?
\end{solution}
\end{truefalse}

%5.1 Q1
\begin{truefalse}
Claim: The expectation of a continuous random variable must always be nonnegative given that the probability density function values are nonnegative. (i.e. , $f(x) \geq 0$).
\begin{solution}
False.\\
The density values are nonnegative. The values that the RV can attain can be negative, therefore the expectation may be negative.
\end{solution}
\end{truefalse}

%5.1 Q2
\begin{truefalse}
Let $X \sim \Norm{0, 1}$. We then know that $\P{X = 0} > \P{X = 5}$.
\begin{solution}
False. By definition $\P{X = k} = 0 \quad \forall k $.
\end{solution}
\end{truefalse}

%5.2 Q1
\begin{truefalse}
Let $U \sim \Unif{(a,b)}$ and  $(c,d) \subset (a,b)$. Then the conditional distribution of $U$ given that $U \in (c,d)$ is $\Unif{(c,d)}$.
\begin{solution}
True.
\\See Blitzstein proposition 5.2.3:\\
\textit{Proof.} For $u$ in $(c, d)$, the conditional CDF at $u$ is
$$
P(U \leq u \mid U \in(c, d))=\frac{P(U \leq u, c<U<d)}{P(U \in(c, d))}=\frac{P(U \in(c, u])}{P(U \in(c, d))}=\frac{u-c}{d-c} .
$$
The conditional CDF is 0 for $u \leq c$ and 1 for $u \geq d$. So the conditional distribution of $U$ is as claimed.
$\square$
\end{solution}
\end{truefalse}

%5.2 Q2
\begin{truefalse}
Let $U \sim \Unif{(a,b)}$. The distribution of the rv $X = c^{2} \log(d) U + e - f^{4}$ is still uniform when $c,d, e, f \in \R^{+}$, and $c\neq 0$, $d\neq 1.$
% you can say the constants to be in N as a version. Or apply a non linear transformation.
\begin{solution}
True.\\
This still is a linear transformation. Let $c^{2} \log(d)$ be a constant $c_{1}$. And $e - f^{4}$ be a constant $c_{2}$. Then the question becomes $c_{1} U + c_{2} $, this still is linear by Blitzstein 5.2.6.
\end{solution}
\end{truefalse}

%5.4 Q1
\begin{truefalse}
Let $Z \sim \Norm{0,1}$ then $\Phi(z) = \Phi(-z)$ due to symmetry.
\begin{solution}
False.\\
This equation for symmetry does not hold for the CDF. \\
We have $\phi(z) = \phi(-z)$
\\And $\Phi(z) = 1 - \Phi(-z)$
\end{solution}
\end{truefalse}

%5.4 Q2
\begin{truefalse}
Let $Z \sim \Norm{\mu, \sigma^{2}}$ with $\sigma>0$.
Let $X = Z \cdot \sigma^{-1} - \mu \cdot \sigma^{-1}$ then $X \sim \Norm{0,1}$.
\begin{solution}
True.
\\Rewriting results in $X = \frac{Z - \mu}{\sigma} $ Then $ X \sim \Norm{0,1}$ by definition.
\end{solution}
\end{truefalse}

%5.5 Q1
\begin{truefalse}
Let $X \sim \Exp{1}$ and $Y = \lambda X$, then $Y \sim \Exp{\lambda}$.
\begin{solution}
False. It should become $Y \sim \Exp{\frac{1}{\lambda}}$.
\end{solution}
\end{truefalse}

%5.5 Q2
\begin{truefalse}
Let $X \sim \Exp{\lambda}$, then:
$$ \int^{\infty}_{0} x  \lambda e^{ - \lambda x} \d x  =  \int^{\infty}_{- \infty} x \lambda e^{ - \lambda x} \1{x> 0}\d x.$$
\begin{solution}
True.\\
\end{solution}
\end{truefalse}

%5.5 Q3
\begin{truefalse}
For the Exponential distribution holds that:
$$\P{X > t + s | X > t} = \P{X > t}$$
\begin{solution}
 False.
 \\It should be :
$\P{X > t + s | X > s} = \P{X > t}$
\\This is the memoryless property.
% A version could be the correct answer, then the solution would be true.
\end{solution}
\end{truefalse}

%5.6 Q1
\begin{truefalse}
Let there be three cars in three different painting workshops.
The painting times of the cars are independent, and start at the same time.
The paint times in hours are iid and follow $X\sim \Exp{\lambda}$, with $\E X = 1/ \lambda = 3$.
Claim: The expected time for the first two cars to be finished is 2.5 hours.

\begin{solution}
True.
Due to independence and memorylessness we have that the expected time for the first two cars to finish is $ T = T_{1} + T_{2}$, as we only need the first two painting jobs;
$T_{1}$ is the time for the first job to finish, $T_{2}$ is the additional time for the second job to finish.
Clearly $T_{1} = \min\{X_{1}, X_{2}, X_{3}\}$,  and, after a restart (recall, memoryless), $T_{2} = \min\{X_{1}, X_{2}\}$. Hence,
\begin{equation*}
\E T = \frac{1}{3 \lambda} + \frac{1}{2\lambda} = 1 + 1.5 = 2.5.
\end{equation*}
For more reference, see 5.6.3 and 5.6.5 in Blitzstein.
\end{solution}
\end{truefalse}



\begin{truefalse}
Let $A$, $B$ be two arbitrary events. Claim: $\P{A|B}=\frac{\P{A \cap B}}{\P{B}}$.
\begin{solution}
False. The condition $\P{B}>0$ is missing.
\end{solution}
\end{truefalse}





\begin{truefalse}
Let $A$, $B$ be events s.t. $\P{A},\P{B}>0$. Claim: $\P{A|B}\P{B}=\P{B|A}\P{A}$.
\begin{solution}
True.
\end{solution}
\end{truefalse}



\begin{truefalse}
Let $A$, $B$, $C$ be events s.t. $\P{A \cap B}>0$. Claim:
\begin{equation}
\frac{\P{B \cap C | A}}{\P{B|A}}=\frac{\P{A \cap C|B}}{\P{A|B}}
\end{equation}
\begin{solution}
True. Both sides are equal to $\P{C|A,B}$.
\end{solution}
\end{truefalse}





\begin{truefalse}
Let $A$, $B$, $C$ be events s.t. $\P{C}>0$. Claim:
\begin{equation}
\P{A \cap B | C} + \P{A \cup B | C}=\P{A|C}+\P{B|C}
\end{equation}
\begin{solution}
True.
\end{solution}
\end{truefalse}


\begin{truefalse}
Let $A$ and $B$ be two disjoint events with positive probability. Claim: $A$ and $B$ are dependent.
\begin{solution}
True. $\P{A\given B} = 0 \neq \P{A}$. %If $A$ and $B$ are disjoint, they can be only independent if  $\P{A}=0$ or $\P{B}=0$.
\end{solution}
\end{truefalse}


\begin{truefalse}
  Suppose $A_i$ for $i=1, 2, \dots, n$ are independent indicator rvs.
  Claim: $\sum_{i=1}^n A_i$ has a Binomial distribution.
\begin{solution}
False. They also need to have identical distributions.
\end{solution}
\end{truefalse}

\begin{truefalse}
Let $X$ and $Y$ be independent. Claim: for any functions $f, g$ it holds that $g(X)$ is independent of $f(Y)$.
\begin{solution}
True.
\end{solution}
\end{truefalse}

\begin{truefalse}
Claim: a discrete random variable requires that the  number of outcomes is finite.
\begin{solution}
False. It must have a \emph{countable} number of outcomes, but not necessarily finite, like $\N$.
\end{solution}
\end{truefalse}

\begin{truefalse}
We have an urn with $w>0$ white balls and $b>0$ black balls.
We pick, without replacement, $n\leq w+b$ balls from the urn.
Then the number $X$ of white balls picked from the urn is hypergeometric, i.e., $X\sim\HGeom{w, b, n}$.

Claim: if  $X \sim \HGeom{3, 5, 2}$, then $\P{X = 3} = 0$.
\begin{solution}
True. $3$ is not in the support of X: $\{0, 1, 2\}$.
\end{solution}
\end{truefalse}

\begin{truefalse}
Given numbers $c, d \in \R$, possibly the same.
Take $X\equiv c$, i.e., $\P{X=c} = 1$, and $Y\equiv d$.
Claim: $X$ and $Y$ are independent.
\begin{solution}
True. $\P{X=c, Y=d} = 1 = \P{X=c}\P{Y=d}$, $\P{X=c, Y\neq d} = 0 = \P{X=c} \P{Y\neq d}$, etc.
\end{solution}
\end{truefalse}

\end{document}
