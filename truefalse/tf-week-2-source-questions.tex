\documentclass[tf-tutorial-all.tex]{subfiles}
\begin{document}



\begin{truefalse}
$X\sim \Geo{p}$. Take $s$ such that $e^{s}q < 1$.
\begin{align}
\label{eq:5}
M_{X}(s)
  &\stackrel{1}= \E{e^{sX}} \stackrel{2}= \sum_{k=1}^{\infty} p q^{k} e^{sk} \\
  &\stackrel{3}= p \sum_{k=1}^{\infty} (e^{s}q)^{k}
  \stackrel{4}= p (\sum_{k=0}^{\infty} (e^{s}q)^{k}-1) \\
&  \stackrel{5}= p /(1+e^{s}q)  - p.
\end{align}

Claim: more than one of these step is incorrect.
\begin{solution}
True. Steps 2 and 5 are incorrect. Step 2: start with $k=0$, step 5: the plus should be a minus.

Variations on this theme.
\begin{enumerate}
\item all steps are correct.
\item step 2 is incorrect.
\end{enumerate}
\end{solution}
\end{truefalse}


\begin{truefalse}
For two strictly positive rvs $X$ and $Y$, let $f_{X,Y}(x, y) = \frac{xy}{x^{2}+y^2}$. Claim: since
\begin{align*}
  f_{X,Y}(x, y) = \frac{xy}{x^{2}+y^2} = \frac{x}{\sqrt{x^{2}+y^2}} \frac{y}{\sqrt{x^{2}+y^2}},
\end{align*}
the rvs $X$ and $Y$ are independent.
\begin{solution}
False.
For indepence of continuous rvs, the joint pdf should be split into two functions that strictly depend on one variable, like $f_{X,Y}(x,y) = f_{X}(x) f_{Y}(y)$.
The two functions at the RHS of the claim are not of this type, both include $x^{2}+y^{2}$.
\end{solution}
\end{truefalse}


\begin{truefalse}
The joint density of $X$ and $Y$ is given by $f_{X,Y}(x,y) = Ce^{-(x + 2y)}\1{x\geq 0}\1{y\geq 0}$.
Claim:  $X$ and $Y$ are iid because  $f_{X,Y}(x, y) = C e^{-x}e^{-2y}$.
\begin{solution}
False. The densities  $f_{X}$ and $f_{Y}$ are not the same.

Variations on this type of question.
\begin{enumerate}
\item Yes, because
\item No,  these rvs are identical, hence dependent.
\item Yes, because $X$ and $Y$ are independent and identical.
\item Yes, because $X$ and $Y$ are independent and identically distributed.
\end{enumerate}
\end{solution}
\end{truefalse}

\begin{truefalse}
$p(k) = \frac{1-x}{1-x^{n+1}} x^k$ with $k \in \{0, \dots, n\}$ is a valid PMF for any $x$.
\begin{solution}
  False. Not if $x = 0$ or $x = 1$.
\end{solution}
\end{truefalse}

\begin{truefalse}
  $M(t) = 2^{-n} \sum_{k=1}^n \binom{n}{k} e^{tk}$ is a valid expression for the MGF of a $\Bin{n, 0.5}$ distribution.
\begin{solution}
True. Apply the binomial theorem: $(e^t + 1)^n = \sum_{k=1}^n \binom{n}{k} e^{tk}$.
\end{solution}
\end{truefalse}

\begin{truefalse}
$\big(\sum_{k=1}^n k \big)^2 = \sum_{k=1}^n k^3$
\begin{solution}
True.
\end{solution}
\end{truefalse}




\begin{truefalse}
Claim: $M_X(s)=e^{-(s-1)^2/2}$ could be a valid MGF for some r.v. $X$.
\begin{solution}
False. Note that $M_X(s)=\E{e^{sX}}$, s.t. $M_X(0)=1$ must always hold.
\end{solution}
\end{truefalse}





\begin{truefalse}
Let $X$ and $Y$ be two independent r.v.s with MGFs $M_X$ and $M_Y$. Claim:
\begin{align}
\label{eq:2}
M_{X-Y}(s)=M_X(s)M_Y(-s)
\end{align}
\begin{solution}
True. $M_{X-Y}(s)=\E{e^{s(X-Y)}}=\E{e^{sX}}\E{e^{-sY}}=M_X(s)M_Y(-s)$.
\end{solution}
\end{truefalse}

%2 6.5
\begin{truefalse}
Let $Z \sim \Norm{0,1}$ and $Y \sim e^{Z}$. We want to find the MGF of Y.
We have the formula:
$$\E{e^{ty}} = \int^{\infty}_{-\infty} \frac{1}{\sqrt{2 \pi}} e^{t e^{z} - \frac{z^{2}}{2}} dz$$
\\ \textbf{Claim: } since the integral does not diverge for t = 0, we can use this as a MGF to estimate moments. 
\begin{solution}
    False.
    \\We need differentiability around zero. Since the integral diverges for t > 0, we have that the MGF is not properly defined for t > 0. As this destroys the differentiability property, we cannot use this as a MGF. 
\end{solution}
\end{truefalse}

\begin{truefalse}
We can obtain the moments for the Log-normal, through the formula given below.
Here $X \sim \Norm{\mu, \sigma^{2}}$ and $Y = e^{X}$
$$E(Y^{n}) =  M_{X}'(n) $$
    \begin{solution}
        False, 
        \\The formula should be:
        $$E(Y^{n}) = \E{e^{n X}} = M_{X}(n) $$
        We do not take the derivative for this moment. 
        $$\E{Y^{n}} = \E{(e^{X}) ^{n}} = \E{e^{nX}} = M_{X}(n)$$
    \end{solution}
\end{truefalse}

%6.6
\begin{truefalse}
Given  $X\sim \Pois{\lambda}$ and $Y \sim \Pois{\mu}$ independently and  $\E{e^{tX}} = e^{-\lambda}\sum^{\infty}_{k=0} \frac{(\lambda e^t)^k}{k!}$
Claim: $M_{X+Y}(t) = e^{(\lambda \mu)(e^t-1)}$
\begin{solution}
It's false. By Taylor series we find$ \E{e^{tX}} = e^{-\lambda} e^{\lambda e^t}$ and by independence $M_{X+Y}(t) = M_X(t) \cdot M_Y(t) = e^{\lambda(e^t-1)} e^{\mu(e^t-1)} = e^{(\mu + \lambda)(e^t-1)}$
\end{solution}
\end{truefalse}


\begin{truefalse}
Given $X_1\sim \Norm{\mu_1, \sigma_1^2}$ with $M_{X_1}(t) = e^{\mu_1t + \frac{1}{2}\sigma_1^2t^2}$.
Given  $X_2\sim \Norm{\mu_2, \sigma_2^2}$
Claim: $M_{X_1+X_2}(t) = e^{(\mu_1 + \mu_2)t +\frac{1}{2}(\sigma_1^2 + \sigma_2^2)t^2}$
\begin{solution}
It's false. It is not given that $X_1$ and $X_2$ are independent.
\end{solution}
\end{truefalse}

\end{document}
