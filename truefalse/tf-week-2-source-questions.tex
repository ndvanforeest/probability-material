\documentclass[tf-tutorial-all.tex]{subfiles}
\begin{document}



\begin{truefalse}
$X\sim \Geo{p}$. Take $s$ such that $e^{s}q < 1$.
\begin{align}
\label{eq:5}
M_{X}(s)
  &\stackrel{1}= \E{e^{sX}} \stackrel{2}= \sum_{k=1}^{\infty} p q^{k} e^{sk} \\
  &\stackrel{3}= p \sum_{k=1}^{\infty} (e^{s}q)^{k}
  \stackrel{4}= p (\sum_{k=0}^{\infty} (e^{s}q)^{k}-1) \\
&  \stackrel{5}= \frac p {1+e^{s}q}  - p.
\end{align}

Claim: more than one of these steps is incorrect.
\begin{solution}
True. Steps 2 and 5 are incorrect. Step 2: start with $k=0$, step 5: the plus should be a minus.

Variations on this theme.
\begin{enumerate}
\item all steps are correct.
\item step 2 is incorrect.
\end{enumerate}
\end{solution}
\end{truefalse}


\begin{truefalse}
For two strictly positive rvs $X$ and $Y$, let $f_{X,Y}(x, y) = \frac{xy}{x^{2}+y^2}$. Claim: since
\begin{align*}
  f_{X,Y}(x, y) = \frac{xy}{x^{2}+y^2} = \frac{x}{\sqrt{x^{2}+y^2}} \frac{y}{\sqrt{x^{2}+y^2}},
\end{align*}
the rvs $X$ and $Y$ are independent.
\begin{solution}
False.
For indepence of continuous rvs, the joint pdf should be split into two functions that strictly depend on one variable, like $f_{X,Y}(x,y) = f_{X}(x) f_{Y}(y)$.
The two functions at the RHS of the claim are not of this type, both include $x^{2}+y^{2}$.
\end{solution}
\end{truefalse}


\begin{truefalse}
The joint density of $X$ and $Y$ is given by $f_{X,Y}(x,y) = Ce^{-(x + 2y)}\1{x\geq 0}\1{y\geq 0}$.
Claim:  $X$ and $Y$ are iid because  $f_{X,Y}(x, y) = C e^{-x}e^{-2y}$.
\begin{solution}
False. The densities  $f_{X}$ and $f_{Y}$ are not the same.

Variations on this type of question.
\begin{enumerate}
\item Yes, because
\item No,  these rvs are identical, hence dependent.
\item Yes, because $X$ and $Y$ are independent and identical.
\item Yes, because $X$ and $Y$ are independent and identically distributed.
\end{enumerate}
\end{solution}
\end{truefalse}

\begin{truefalse}
Claim: $p(k) = \frac{1-x}{1-x^{n+1}} x^k$ with $k \in \{0, \dots, n\}$ is a valid PMF for any $x$.
\begin{solution}
  False. Not if $x = 0$ or $x = 1$.
\end{solution}
\end{truefalse}

\begin{truefalse}
  Let $X \sim \Geo{p}$. Claim: all of the following steps are correct.
  \begin{equation*}
    \P{X = k \given X \geq n}
    = \frac{\P{X=k, X \geq n}}{\P{X \geq n}}
    = \frac{\1{k \geq n} \P{X = k}}{\P{X \geq n}}
    = \1{k \geq n} pq^{k-n}
  \end{equation*}
  Thus,
  \begin{align*}
    \E{X \given X \geq n} &= \sum_{k=0}^\infty k \1{k \geq n} pq^{k-n}\\
    &=  p \sum_{k=n}^\infty kq^{k-n}\\
    &= (1 - q) \big(n q^0 + (n+1) q^1 + (n+2) q^2 + \dots\big)\\
    &= \big(n q^0 + (n+1) q^1 + (n+2) q^2 + \dots\big) - \big(n q^1 + (n+1)q^2 + (n+2)q^3 + \dots\big)\\
    &= n + q^1 + q^2 + \dots\\
    &= n + \frac{1}{1-q} - 1\\
    &= n + \frac{q}{p}\\
  \end{align*}
\begin{solution}
    True. Of course, the memoryless property of the geometric
    distribution can be used to find the answer too.
\end{solution}
\end{truefalse}


\begin{truefalse}
Claim:  $M(t) = 2^{-n} \sum_{k=0}^n \binom{n}{k} e^{tk}$ is a valid expression for the MGF of a $\Bin{n, 0.5}$ distribution.
\begin{solution}
True. Apply the binomial theorem: $(e^t + 1)^n = \sum_{k=0}^n \binom{n}{k} e^{tk}$.
\end{solution}
\end{truefalse}

\begin{truefalse}
 Claim:  This contains an error:
  \begin{equation*}
e \stackrel 1 =  \lim_{n\to\infty} \big(1+n^{-1}\big)^n   \stackrel{2} = \lim_{n\to\infty} \sum^n_{k=0} n^{-k}.
  \end{equation*}
\begin{solution}
True, Step 2 is wrong. Compare the Taylor series for $e^{x}$.
\end{solution}
\end{truefalse}

\begin{truefalse}
Claim: suppose $f(x) = a x + b$ with $a\neq 0$, then there is a $c$ such that $c e^{-(f(x))^2}$ is the pdf of a normal distribution.
\begin{solution}
It's true.
\end{solution}
\end{truefalse}


\begin{truefalse}
Claim: $M_X(s)=e^{-(s-1)^2/2}$ could be a valid MGF for some rv $X$.
\begin{solution}
False. Note that $M_X(s)=\E{e^{sX}}$, s.t. $M_X(0)=1$ must always hold.
\end{solution}
\end{truefalse}


\begin{truefalse}
Let $X$ and $Y$ be two independent rvs such that $\E{e^{s X}}$ and $\E{e^{s Y}}$ are well defined for $s$ in an open interval around $0$.
Claim:
\begin{align}
\label{eq:2}
M_{X-Y}(s)=M_X(s)M_Y(-s)
\end{align}
\begin{solution}
True. $M_{X-Y}(s)=\E{e^{s(X-Y)}}=\E{e^{sX}}\E{e^{-sY}}=M_X(s)M_Y(-s)$.
\end{solution}
\end{truefalse}

\begin{truefalse}
Given $X\sim \Pois{\lambda}$, $Y \sim \Pois{\mu}$, $X$ and $Y$ are independent, and $\E{e^{tX}} = e^{-\lambda}\sum^{\infty}_{k=0} \frac{(\lambda e^t)^k}{k!}$.
Claim: $M_{X+Y}(t) = e^{(\lambda \mu)(e^t-1)}$.
\begin{solution}
It's false. By Taylor series we find$ \E{e^{tX}} = e^{-\lambda} e^{\lambda e^t}$ and by independence $M_{X+Y}(t) = M_X(t) \cdot M_Y(t) = e^{\lambda(e^t-1)} e^{\mu(e^t-1)} = e^{(\mu + \lambda)(e^t-1)}$
\end{solution}
\end{truefalse}


\begin{truefalse}
Given $X_1\sim \Norm{\mu_1, \sigma_1^2}$ with $M_{X_1}(t) = e^{\mu_1t + \frac{1}{2}\sigma_1^2t^2}$, and $X_2\sim \Norm{\mu_2, \sigma_2^2}$.
Claim: $M_{X_1+X_2}(t) = e^{(\mu_1 + \mu_2)t +\frac{1}{2}(\sigma_1^2 + \sigma_2^2)t^2}$.
\begin{solution}
It's false. It is not given that $X_1$ and $X_2$ are independent.
\end{solution}
\end{truefalse}

\end{document}
