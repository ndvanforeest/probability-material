% arara: pdflatex: { shell: yes }
% arara: pythontex: {verbose: yes, rerun: always }
% arara: pdflatex: { shell: yes }

\input{header}

\title{Probability Distributions, Week 1}

\begin{document}
\maketitle

\toccontents
%\tableofcontents

\section{Lecture 1}


\subsection{Compulsory material}

\begin{enumerate}
\item BH Book: Read Section 6.4, MGFs
\item BH Book: Read Section 6.5, Generating moments with MGFs
\item BH Book: Read Section 6.6, Sums of independent rvs via MGFs
\item BH Book: Read Appendix 9
\item BH video: 18
\end{enumerate}


Here is a story to see how the Poisson distribution emerges, just by allowing people a bit of choice.
The conclusion is that the Poisson distribution comes up at many unexpected processes. 'A patient arrives at the first aid of a hospital with, perhaps, a broken foot.
An X-ray photo shows that the patient indeed has a broken foot.
Given the type of fraction, the patient has to undergo surgery somewhere in the next 10 days.
Should we let the patient decide what is the best day to do the surgery?

We model the system as a queueing network with two stations, the first is the X-ray, the second the surgery.
We make the assumption that the first station works super regularly and sends exactly five patients every day to the surgery.
(Of course these assumptions are a bit silly, but we want to demonstrate a certain phenomenon.)
After the first step, each patient is allowed to choose an arbitrary day in the next $n$ days for the next appointment.
What is the distribution of the number of appointments on a certain day?
Interestingly, this will be approximately Poisson!
Here we use moment generating functions (MGFs) to see why. (To avoid some difficult behavior due to initial effects, we assume that the system has been operational for a long time.)

\begin{exercise}
Show that the MGF of $N\sim \Pois{\lambda}$ is $M_{N}(s)= e^{\lambda(e^s-1)}$.
\begin{solution}
Read BH chapter 6 to find out where. While reading you'll discover lots of other relevant and interesting material.
\end{solution}
\end{exercise}

Next, let \(X_{i}\in\{1, 2, \ldots, n\}\) be the chosen day of patient \(i\).
Then \(Y_{i}(j)=\1{X_{i}=j}\) is 1 if \(X_i=j\) and 0 else.
Thus, when we have \(m\) patients, \(S_{m}=\sum_{i=1}^{m} Y_i(j)\) is the number of jobs will arrive on day \(j\).
(In the above queueing model, there are 5 arrivals per day, hence \(m=5n\).) We are interested in the distribution of the number of arrivals on some given day $j$, when $n$ is large.

\begin{exercise}
Show that the MGF of \(Y\), i.e., the generic rv associated to \(Y_j(\cdot)\), is
\begin{equation}
\label{eq:2}
M_Y(s) = \E{e^{s Y}}  = 1 + \frac{e^s-1}{n}.
\end{equation}
\begin{solution}
\begin{equation*}
M_Y(s) = \E{e^{s Y}} = e^{s\cdot 1} \frac{1}{n} + e^{s\cdot 0} \frac{n-1}{n} = 1 + \frac{e^s-1}{n},
\end{equation*}
since \(\P{Y_j=1} = 1/n = 1-\P{Y=0}\).
\end{solution}
\end{exercise}

\begin{exercise}
Show that
\begin{equation}
M_{S_{m}}(s) = \left( 1 + \frac{e^s-1}{n}\right)^{5n}.
\end{equation}
\begin{solution}
Using that  \(Y_i(j)\) are idd,
\begin{equation}
M_{S_{m}}(s) = \left( M_Y(s)\right)^m = \left( 1 + \frac{e^s-1}{n}\right)^{5n}.
\end{equation}
\end{solution}
\end{exercise}

\begin{exercise}
When \(n\) is not too small, use a standard limit to see that
\begin{equation}
M_S(s) \approx \exp(5(e^s-1)).
\end{equation}
Observe that the RHS the MGF of the Poisson distribution with mean \(\lambda = 5\).
\begin{solution}
Check out appendix A of BH to conclude that
\begin{equation}
\lim_{n\to\infty}\left( 1 + \frac{e^s-1}{n}\right)^{5n} =\exp(5(e^s-1))
\end{equation}
\end{solution}
\end{exercise}

\begin{exercise}
Is it a good idea to let patients select the day at which they want to receive surgery?
\begin{solution}
No, because it gives lots of variability.
The demand at the surgery department will be about Poisson, so some days there will be many patients, and on some other hardly any.
\end{solution}
\end{exercise}

\begin{exercise}
Read BH.6.22. Relate that exercise to the story of the patients of this lecture. You should see that conceptually the mapping is one-to-one.
\begin{solution}
Just read the solution of BH.6.22 and compare.
\end{solution}
\end{exercise}

\begin{pycode}
from pathlib import Path

exercise_name = "bh-6.24.tex"
fname = Path("../bh_problems") / exercise_name
with fname.open("r") as fp:
    state = 0  # dump
    for line in fp.readlines():
        if line[:16] == r"\begin{exercise}":
            state = 1
        if state == 1:
            print(line.strip())
        if line[:14] == r"\end{exercise}":
            break
\end{pycode}



\section{Lecture 2}
\label{sec:lecture-1}

\subsection{Compulsory material}

\begin{enumerate}
\item BH Book: Read Section 7.0 intro,
\item BH Book: Read Section 7.1.1 joint, marginal, conditional for discrete rvs
\item BH video: 19
\end{enumerate}

% \begin{exercise}
% BH Exercise 7.9
% \begin{solution}
% See our  solution manual of BH exercises on Brightspace.
% \end{solution}
% \end{exercise}

\begin{pycode}
from pathlib import Path

exercise_name = "bh-7.9.tex"
fname = Path("../bh_problems") / exercise_name
with fname.open("r") as fp:
    state = 0  # dump
    for line in fp.readlines():
        if line[:16] == r"\begin{exercise}":
            state = 1
        if state == 1:
            print(line.strip())
        if line[:14] == r"\end{exercise}":
            break
\end{pycode}

\begin{pycode}
from pathlib import Path

exercise_name = "bh-7.29.tex"
fname = Path("../bh_problems") / exercise_name
with fname.open("r") as fp:
    state = 0  # dump
    for line in fp.readlines():
        if line[:16] == r"\begin{exercise}":
            state = 1
        if state == 1:
            print(line.strip())
        if line[:14] == r"\end{exercise}":
            break
\end{pycode}


\begin{exercise}
Consider 12 football players on a football field.
Eleven of them are players of F.C. Barcelona, the other one is an arbiter.
We select a random player, uniform.
This player must take a penalty.
The probability that a player of Barcelona scores is 70\%, for the arbiter it is 50\%.
Let $P\in \{A, B\}$ be the rv that corresponds to the selected player, and $S\in\{0,1\}$ be the score.
\begin{enumerate}
\item What is the PMF? In other words, determine $\P{P = B, S=1}$ and so on for all possibilities.
\item What is $\P{S=1}$? What is $\P{P=B}$?
\item Show that $S$ and $P$ are dependent.
\end{enumerate}
\begin{solution}
Here is the joint PMF:
\begin{align}
\P{P=A, S=1} &= \frac{1}{12}0.5 & \P{P=A, S=0} &= \frac{1}{12}0.5 \\
\P{P=B, S=1} &= \frac{11}{12}0.7 & \P{P=B, S=0} &= \frac{11}{12}0.3.
\end{align}
Now the marginal PMFs
\begin{align*}
\P{S=1}  &= \P{P=A, S=1} + \P{P=B, S=1} = 0.042 + 0.64 = 0.683 = 1-\P{S=0}\\
\P{P=B}  &= \frac{11}{12} = 1-\P{P=A}.
\end{align*}
For independence we take the definition.
In general, for all outcomes $x,y$ we must have that $\P{X=x, Y=y} = \P{X=x}\P{Y=y}$.
For our present example, let's check for  a particular outcome:
\begin{align*}
\P{P=B, S=1} &= \frac{11}{12}\cdot0.7 \neq \P{P=B}\P{S=1} = \frac{11}{12} \cdot 0.683
\end{align*}
The joint PMF is obviously not the same as the product of the marginals, which implies that $P$ and $S$ are not independent.
\end{solution}
\end{exercise}


\section{Tutorial}
\label{sec:tutorial}

\subsection{Practice}

Practice exercises are meant to get you started with reading definitions, notation, and involve only simple calculus.
These questions are simpler than the exercises of BH.

\begin{exercise}
Let $L=\min\{X, Y\}$, where $X, Y\sim \Geo{p}$ and independent.
What is the support of $L$? Then, use the fundamental bridge and 2D LOTUS to show that
\begin{equation*}
\P{L\geq i}=q^{2i} \implies L\sim\Geo{1-q^{2}}.
\end{equation*}
\begin{hint}
The fundamental bridge and 2D LOTUS have  the general form
\begin{equation*}
\P{g(X,Y)\in A} = \E{\1{g(X, Y) \in A}} = \sum_{i}\sum_j \1{g(i, j)\in A} \P{X=i, Y=j}.
  \end{equation*}
Take  $g(i,j) = \min\{i, j\}$.
\end{hint}
\begin{solution}
With the hint,
  \begin{align*}
\P{L\geq k} = \P{\min(X, Y) \geq k}
&= \sum_{i=0}^\infty \sum_{j=0}^\infty \1{\min\{i,j\}\geq k} \P{X=i, Y=j}\\
&= \sum_{i=k}^\infty \sum_{j=k}^\infty \P{X=i}\P{Y=j} = \sum_{i=k}^\infty \sum_{j=k}^\infty p^2q^{i + j} \\
&= p^2 \big ( q^k \sum_{i=0}^\infty q^i \big) \big( q^k \sum_{j=0}^\infty q^j \big) = \frac{p^2}{(1 - q)^2} q^{2k} = q^{2k}
  \end{align*}
In the second line we use the fact that $X$ and $Y$ are independent to infer that their joint PMF is the product of the marginal PMF's.
Furthermore, whenever the value of $j$ or $i$ is less than $k$ the indicator is zero, hence the change in the lower-bounds.
In the third line, constants are taken out of the sum to easily recognize patterns (the geometric sum),
then a typical procedure is used to get the full geometric sum: $\sum_{j=k}^\infty q^j = q^k + q^{k+1} + \dots = q^k (q^0 + q^1 + \dots)$.

$\P{L\geq i}$ has the same form as $\P{X\geq i}$, but now with
$q^{2i}$ rather than $q^{i}$. Since the CDF fully determines
distribution, we can conclude $L\sim \Geo{q^2}$.

The Fundamental Bridge + Lotus approach is very general and the
indicators work as bookkeeping devices, reducing the chances of making
mistakes.

We can also use some common tricks to solve this particular problem which are worth mentioning.
First of all, convince yourself of the equality of these sets $\{\min\{X, Y\} > k\} = \{X > k \cap Y > k\}$.
Similarly $\{\max\{X, Y\} < k\} = \{X < k \cap Y < k\}$.
Then using the independence of $X$ and $Y$, we can quickly find the answer:
\begin{equation*}
\P{\min\{X, Y\} \geq k} = \P{X\geq k}\P{Y\geq k} = q^k q^k = q^{2k}
\end{equation*}
Notice the similarity between the two solutions, the first approach
just works out a few more steps. However, the first approach also
works for situations where $X$ and $Y$ are not independent and for all
kinds of functions of $X$ and $Y$.
\end{solution}
\end{exercise}

\begin{exercise}\label{ex:1}
Let $M=\max\{X, Y\}$, where $X, Y\sim \Geo{p}$ and independent.
Show that
\begin{align*}
\P{M=k} = 2 pq^{k}(1-q^{k}) + p^2q^{2k}.
\end{align*}
\begin{hint}
  Use 2D LOTUS on $g(x,y) = \1{\max\{x, y\} = k}$.
\end{hint}
\begin{solution}
  \begin{align*}
\P{M=k}
&= \P{\max\{X, Y\} = k} \\
&=p^{2}\sum_{ij}\1{\max\{i,j\} =k} q^i q^j\\
&\stackrel{1}{=}2 p^{2}\sum_{ij}\1{i=k}\1{j < k} q^i q^j + p^{2}\sum_{ij}\1{i=j=k} q^{i} q^j \\
&=2 p^{2}q^{k}\sum_{j < k} q^j + p^{2}q^{2k}\\
&=2 p^{2}q^{k}\frac{1-q^{k}}{1-q} +  p^{2}q^{2k}\\
  \end{align*}
As $X$ and $Y$ are idd we use symmetry in Step 1.
Here indicator variables are used as a bookkeeping device for conditioning.

A cool trick for discrete rvs that can be used to solve this problem
is that $\P{M=k} = \P{M \leq k} - \P{M < k}$. This can be seen from
$\P{M < k} = \sum_{j=-\infty}^{k-1} \P{M=j}$, or by using $\P{M=k}
= \P{M\leq k \cap M\geq k}$ and $\P{M\leq k \cup M \geq k} = \P{M\leq
k} + \P{M \geq k} - \P{M\leq k \cap M\geq k} = 1$. Then using the
independence of $X$ and $Y$:

\begin{align*}
\P{M = k} &= \P{M \leq k} - \P{M < k} = \P{X \leq k} \P{Y \leq k} - \P{X < k} \P{Y < k} \\
&= (1-q^{k+1})^2 - (1 - q^k)^2 = (1 - q^k + pq^k)^2 - (1-q^k)^2 = (pq^k)^2 + 2(1-q^k)pq^k
\end{align*}

\end{solution}
\end{exercise}

\begin{exercise}\label{ex:2}
Explain that
\begin{equation*}
\P{L=i, M=k} = 2p^{2}q^{i+k}\1{k>i}+ p^{2}q^{2i}\1{i=k}.
\end{equation*}
\begin{solution}
  \begin{align*}
\P{L=i, M=k}
&= 2\P{X=i, Y=k}\1{k>i} + \P{X=Y=i}\1{i=k} \\
&= 2p^{2}q^{i+k}\1{k>i}+ p^{2}q^{2i}\1{i=k}.
  \end{align*}
\end{solution}
\end{exercise}

\begin{exercise}
With the previous exercise, use marginalization to compute the marginal PMF $\P{M=k}$.
\begin{solution}
  \begin{align*}
\P{M=k}
&= \sum_{i} \P{L=i, M=k} \\
 &= \sum_{i} (2p^{2}q^{i+k}\1{k>i}+ p^{2}q^{2i}\1{i=k}) \\
 &= 2p^2q^k\sum_{i=0}^{k-1} q^{i}+ p^{2}q^{2k} \\
 &= 2pq^k (1-q^{k})+ p^{2}q^{2k} \\
 &= 2pq^k + (p^{2}-2p) q^{2k},
  \end{align*}
\end{solution}
\end{exercise}


\subsection{BH Exercises}
\label{sec:bh-exercises}

\begin{pycode}
from pathlib import Path

exercise_name = "bh-6.22.tex"
fname = Path("../bh_problems") / exercise_name
with fname.open("r") as fp:
    state = 0  # dump
    for line in fp.readlines():
        if line[:16] == r"\begin{exercise}":
            state = 1
        if state == 1:
            print(line.strip())
        if line[:14] == r"\end{exercise}":
            break
\end{pycode}



\section{Homework}
\label{sec:homework}

\subsection{Practice}
\label{sec:simple-exercises}

Practice exercises are meant to get you started with reading definitions, notation, and involve only simple calculus.
These questions are simpler than the exercises of BH.



\begin{exercise}
In your own words, explain what is
\begin{enumerate}
\item a joint PMF, PDF, CDF;
\item a conditional PMF, PDF, CDF;
\item a marginal PMF, PDF, CDF.
\end{enumerate}
\begin{solution}
Check the definitions of the book.

Mistake: To say that $\P{X=x}$ is the PMF for a continuous random variable is wrong, because $\P{X=x}=0$ when $X$ is continuous.

Why is $\P{1 < x \leq 4}$ wrong notation?
hint: $X$ should be a capital.
What is the difference between $X$ and $x$?

\end{solution}
\end{exercise}



\begin{exercise}
Suppose the probability of obtaining a head twice out of two coin flips is $\P{X_1=H, X_2=H}$.
What has this to do with joint PMFs? Can you generalize this idea to other examples?
\begin{solution}
This example shows why joint distributions are important!
In any experiment that involves a sequence of measurements, such as multiple throws of a coin, or the weighing of a bunch of chimpanzees, we have to deal with joint CDFs and PMFs.
\end{solution}
\end{exercise}


\begin{exercise}
In the previous exercise, suppose the outcome of the second throw is always equal to that of the first. Specify the joint PMF.
\begin{solution}
Here, we deal with two rvs, and we have to specify how they depend. In the present case $\P{X_1= H, X_2=H} = \P{X_1=H}$ and $\P{X_1= T, X_2=T} = \P{X_1=T}$, $\P{X_1= H, X_2=T} = \P{X_1= T, X_2=H} = 0$. Note that with this, we specified the joint PMF on all possible outcomes.
\end{solution}
\end{exercise}


\begin{exercise}
Let $X$ be uniformly distributed on the set $\{0,1,2\}$ and let $Y \sim \Bern{1/4}$; $X$ and $Y$ are independent.
\begin{enumerate}
\item Present a contingency table for $X$ and $Y$.
\item What is the interpretation of the column sums of the table?
\item What is the interpretation of the row sums of the table?
\item Suppose you would change some of the entries in the table. Are $X$ and $Y$ still independent?
\end{enumerate}
\begin{solution}
$\P{X=0, Y=0} = 1/3 \cdot 3/4$,
$\P{X=0, Y=1} = 1/3 \cdot 1/4$, and so on.

If we have one column with $Y=0$ and the other with $Y=1$, then the sum over the columns are $\P{Y=0}$ and $\P{Y=1}$. The row sum for row $i$ are  $\P{X=i}$.

Changing the values will (most of the time) make $X$ and $Y$ dependent. But, what if we changes the values such that  $\P{X=0, Y=0} =1$? Are $X$ and $Y$ then again independent? Check the conditions again.
\end{solution}
\end{exercise}


\begin{exercise}
A machine makes items on a day.
Some items, independent of the other items, are failed (i.e., do not meet the quality requirements).
What are $N$ and  $p$ in the context of the chicken-egg story of BH? What are the `eggs' in this context, and what is the meaning of `hatching'?
What type of `hatching' do we have here?
\begin{solution}
  The number of produced items (laid eggs) is $N$. The probability of hatching is $p$, that is, an item is OK. The hatched eggs are the good items.
\end{solution}
\end{exercise}

% \begin{exercise}
% Apply the chicken-egg story. Families enter a zoo in a given hour. Some families have one child, other two, and so on.
% What are the `eggs' in this context, and what is the meaning of `hatching'?
% \end{exercise}


\begin{exercise}
Theorem 7.1.11. What is the meaning of the notation $X|N=n$?
\begin{solution}
  Given $N=n$, the random variable $X$ has a certain distribution, here binomial.
\end{solution}
\end{exercise}






\begin{exercise}
An insurance company receives on a certain day two claims $X, Y \geq 0$.
We will find the PMF of the loss $Z=X+Y$ under different assumptions.

Suppose $p_{X,Y}(i,j) = c \1{i=j}\1{1\leq i \leq 4}$.
\begin{enumerate}
\item  What is $c$?
\item What is $F_{X}(i)$?
\item What is $F_{Y}(j)$?
\item  Are $X$ and $Y$ dependent?  If so, why, because $1=F_{X,Y}(4,4)= F_X(4)F_Y(4)$?
\item What is $\P{Z=k}$?
\item What is $\V Z$?
\end{enumerate}


\begin{solution}
1. $c=1/4$ because there are just four possible values for $i$ and $j$.
2. and 3. Use marginalization:
\begin{align}
F_X(k) &=  F_{X,Y}(k, \infty ) = \sum_{i\leq k} \sum_j p_{X,Y}(i,j) \\
 &= \frac{1}{4}\sum_{i\leq k} \sum_j \1{i=j}\1{1\leq i \leq 4}\\
 &= \frac{1}{4}\sum_{i\leq k} \1{1\leq i \leq 4} \\
&=k/4,\\
F_Y(j) &= j/4.
\end{align}

4.  The equality in the question must hold for all $i,j$, not only for $i=j=4$.
  If you take $i=j=1$, you'll see immediately that $F_{X,Y}(1,1)\neq F_X(1)F_Y(1)$:
  \begin{align}
    \label{eq:823}
    \frac{1}{4} = F_{X,Y}(1,1) \neq F_{X}(1) F_Y(1) = \frac{1}{4}\frac{1}{4}.
  \end{align}

5. $\P{Z=2} = \P{X=1, Y=1} = 1/4 = \P{Z=4}$, etc.
$\P{Z=k} = 0$ for $k\not \in \{2, 4, 6, 8\}$.

6.
Here is one approach
\begin{align}
\label{eq:83}
\V Z &= \E{Z^2} - (\E Z)^{2}\\
\E{Z^2} &= \E{(X+Y)^{2}} = \E{X^{2}} + 2\E{XY} + \E{Y^{2}} \\
(E{Z})^{2} &= (\E X + \E Y)^{2} \\
 &= (\E X)^2 + 2\E X \E Y + (\E Y)^{2} \\
&\implies \\
\V Z &= \E{Z^2} - (\E Z)^{2}\\
 &= \V X + \V Y + 2 (\E{XY} - (\E X \E Y))\\
\E{XY} &= \sum_{ij} i j p_{X,Y}(i,j) = \frac 1 4 (1 + 4 + 9 + 16) = \ldots \\
\E{X^{2}} &= \ldots
\end{align}
The numbers are for you to compute.
\end{solution}
\end{exercise}



\subsection{BH Exercises}
\label{sec:bh-exercises-1}


\begin{pycode}
from pathlib import Path

exercise_name = "bh-6.13.tex"
fname = Path("../bh_problems") / exercise_name
with fname.open("r") as fp:
    state = 0  # dump
    for line in fp.readlines():
        if line[:16] == r"\begin{exercise}":
            state = 1
        if state == 1:
            print(line.strip())
        if line[:14] == r"\end{exercise}":
            break
\end{pycode}

\begin{pycode}
from pathlib import Path

exercise_name = "bh-6.18.tex"
fname = Path("../bh_problems") / exercise_name
with fname.open("r") as fp:
    state = 0  # dump
    for line in fp.readlines():
        if line[:16] == r"\begin{exercise}":
            state = 1
        if state == 1:
            print(line.strip())
        if line[:14] == r"\end{exercise}":
            break
\end{pycode}

% \section{Poll Everywhere questions}
% \label{sec:polev-quest}

% To be included after the tutorial.

\section{Assignment}
\label{sec:assignment}

\setcounter{theorem}{0}
\subfile{../assignments/bh-7-9.tex}
%\subfile{../assignments/bh-7-9b.tex}


\section{True false questions}
\label{sec:true-false-questions}
\setcounter{theorem}{0}
\subfile{../truefalse/tf-week-1-source-questions.tex}



%\opt{all-solutions-at-end}{
\Closesolutionfile{hint}
\Closesolutionfile{ans}
\clearpage
\section{Hints}
\input{hint}
\clearpage
\section{Solutions}
\input{ans}
%}

\end{document}
