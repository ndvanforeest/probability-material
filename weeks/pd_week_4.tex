% arara: pdflatex: { shell: yes }
% arara: pythontex: {verbose: yes, rerun: always }
% arara: pdflatex: { shell: yes }

\input{header}

\title{Probability Distributions, Week 4}

\begin{document}
\maketitle
\toccontents

\section{Lecture 7}
\label{sec:lecture-1}


\subsection{Compulsory material}
\label{sec:compulory-material}

\begin{itemize}
\item 9.1: read, but skip example 9.1.6 on the two envelopes paradox. For example BH.9.1.7, see below.
\item BH video: 26
\end{itemize}




Conditioning, LOTP, and recursion can go hand in hand. As such, conditioning is a great and elegant tool to simplify probabilistic problems. This lecture focusses on demonstrating how to use these ideas, use the notation correctly, and present a new distribution.

To get the mood, here is first a short fun question on how to use recursion.

\begin{exercise}
We have a chocolate bar consisting of $n$ small squares.
The bar is in any shape you like, square, rectangular, whatever.
What is the number of times you have to break the bar such that you end up with the $n$ single pieces?
%Recursive arguments are very powerful, and I often find them very insightful (and I also like this type of reasoning).
\begin{solution}
  Write $T(n)$ for the number of times you have to break the bar when there are $n$ squares.
 Suppose $n=k+m$, $1 \leq k < n$. Then it must hold that $T(n) = T(k) + T(m) + 1$, because we need $T(k)$ to break the part with $k$ pieces, and $T(m)$ for the part with $m$ pieces, and we need 1 breakage to break the big bar, consisting of $n$ parts, into the two parts. But then, with the \emph{boundary condition} $T(1)=0$,
  \begin{align*}
    T(2) &= T(1) + T(1) + 1 = 1, \\
T(3) &= T(1) + T(2) + 1 = 1 + 1 = 2,
  \end{align*}
and so on. So we guess that $T(n)=n-1$ for any $n$. Let's check. It certainly holds for $n=1$.  Generally,
\begin{align*}
n-1 = T(n) = T(k) + T(m) + 1 = k-1 + m-1 + 1 = k+m-1 = n-1.
\end{align*}
We have solved the recursion, i.e., $T(n)=n-1$, and now we know that in whatever way we break an $n$ square chocolate bar with whatever shape, we need $n-1$ breackages.

\end{solution}
\end{exercise}


Here is the exercise in which we apply recursion to the negative hypergeometric distribution.

\begin{exercise}
Write $T(b,w)$ for the number of black balls drawn at random without replacement from an urn containing $w\geq 1$ white balls and $b$ black balls \emph{before} we draw 1 white ball. Use recurstion to show that
\begin{align}
  \label{eq:727}
\E{T(b,w)} = \frac{b}{w+1}.
\end{align}
In other words, the question is to use recursion to compute  the expectation of a negative hypergeometric distribution with parameters $w,b$ and $r=1$.
It is convenient to write $N(b,w) = \E{T(b,w)}$; it saves ink and it gives more overview of what we do.

Compare this with the geometric random variable: then we ask how often to throw, for instance, a coin \emph{before} we see tails.
The coin can be seen as an urn with an infinite number of white balls (tails) and an infinite number of black ball (heads) but such that the ratio of black and white balls is $p=w/(b+w)$.
Hence, the negative hypergeometric and geometric distributions are closely related.

\begin{hint}
Explain that
\begin{align}
N(0, w) &= 0\quad \text{ for all $w$},\\
  N(b,w) &= \frac{b}{b+w} (1+N(b-1, w)).
\end{align}
Then show that this implies that $N(b,w) = b/ (w+1)$.
\end{hint}
\begin{solution}
Once again, realize that as soon as we pick a white ball, we have to stop.

If there are no black balls left in the urn,  we cannot pick a black ball, hence $N(0,w) = 0$.

When $w, b \geq 1$ two things can happen.
Suppose we pick a white ball, then we stop right away.
Suppose we pick a black ball, which happens with probability $b/(b+w)$, we obtain one black ball, and we continue with one black ball less.
Hence, with LOTE and $X_{1}$ the color of the ball we pick,
\begin{align}
\E{T(b,w)}
  &= \E{T(b,w) | X_1 = w} \P{X_1=w} + \E{T(b,w)|X_1=b} \P{X_1=b}   \\
  &= 0 + \E{T(b,w)|X_1=b} \P{X_1=b} \\
  &= (1+\E{T(b-1, w)}) \frac{b}{b+w},
\end{align}
because $\E{T(b, w) |X_1=b} = 1 + \E{T(b-1,w)}$, i.e., we picked a black ball from the urn.

In our notation, we can write this a bit shorter:
\begin{equation}\label{eq:2}
N(b,w) = \frac{b}{w+b} (1+N(b-1, w)) = a + a N(b-1, w),
\end{equation}
where we write $a=b/(b+w)$ for ease and overview.


Now we need a real tiny bit of inspiration.
The equation for $N(b,w)$ looks like a line dependent on $b$ and with slope $a$. (In words, we see in the expression for $N(w, b)$ that the $b$ reduces by 1, but $w$ remains the same.)
So, let's \emph{guess} that $N(w,b)=\alpha b + \beta$ for some $\alpha>0$ and $\beta$.
Guessing is always allowed; if it works, we are done (since the recursion, together with the boundary condition, has a unique solution: just repeatedly applying it allows us to calculate all values).
In general, you can make any guess whatsoever, but mind that only the good guesses work.

The value for $\beta$ follows directly from the boundary condition $N(0, w) = 0$ (we cannot pick a black ball if $b=0$). So $\alpha \cdot 0 + \beta = 0 \implies \beta = 0$.

Continuing with the form $N(b, w) = \alpha b$ in the above equation for $N(b,w)$, we find that
\begin{equation*}
\alpha b = \frac{b}{w+b} (1+ \alpha (b-1)) \stackrel{\textrm{\tiny algebra}}{\implies} \alpha = \frac{1}{w+1} \implies  N(b,w) = \frac{b }{w+1}.
\end{equation*}
This is consistent with the boundary condition $N(0,w)=0$.
Hence, we can move from the case $b=0$ to the case with $b=1$, and so on, thereby proving the validity of the formula in general.
\end{solution}
\end{exercise}

\begin{exercise}
We extend the previous exercise to a case in which we stop until we picked $r$, $r\geq 1$, white balls from the urn. Thus, we like to know the number of black balls drawn before we picked $r$ white balls.

For this, write $N_{r}(w,b)$ for an urn with $w$ white balls and $b$ black balls, and $r$ white balls to go.
\begin{hint}
Explain that $N_{0}(w,b) = 0$ and
\begin{align}
  N_r(b,w) &= \frac{w}{w+b} N_{r-1}(b,w-1) +  \frac{b}{w+b} (1+N_{r}( b-1, w)).
\end{align}
Then show that this implies that $N_{r}(b, w) = r b/ (w+1)$.

Note that this is consistent with the previous exercise.

\end{hint}
\begin{solution}
Use the hint!

The recursion follows right away by noticing that we pick a white ball with probability $w/(w+b)$, but then we remove one white ball \emph{and} we have one white ball less to go.
With probability $b/(w+b)$ we pick a black ball, in which case the number of black balls drawn increases by one, but there is one black ball less in the urn.

The boundary condition is clear: $N_{0}(b,w)=0$ because we stop when $r=0$.
Note that this is consistent with the previous exercise.

If $r=2$, we use from the previous exercise that
\begin{equation*}
N_{r-1}(w-1, b) = N_{1}(w-1, b) = \frac{b }{w-1 + 1} = \frac{b }{w}.
\end{equation*}
Using this in the recursion,
\begin{align*}
  N_2(b,w)
&= \frac{w}{w+b} N_{1}(b, w-1) +  \frac{b}{w+b} (1+N_{2}(b-1,w)) \\
&= \frac{w}{w+b} \frac{b }{w} +  \frac{b}{w+b} (1+N_{2}(b-1), w) \\
&=  \frac{b}{w+b} (2+N_{2}(b-1, w)).
\end{align*}
But this has the same form as what we see in the previous exercise, except that the 1 has been replaced by a 2. But, now we are dealing with the case $r=2$ instead of $r=1$! Hence, by the same line of reasoning,
\begin{equation*}
  N_2(b,w) = 2 \frac{b}{w+1}
\end{equation*}
Knowing the result for $r=2$, we fill this for $r=3$, and so on, to get the general result.

What an elegant procedure;  we pull ourselves out of the swamp, just like Baron Munchhausen.
\end{solution}
\end{exercise}

\begin{remark}
Consider Example BH.9.1.7 on the mystery prize.
I (=NvF) like such problems a lot: it deals with optimal control (what is the height of the bid that maximizes the profit?)
in a stochastic environment.
However, I find the solution method of the book somewhat hard to understand.
I would use the tools of Section BH.9.2 directly; perhaps you find it simpler too.

The payoff $W$ is the money I get if my bid $b$ is accepted, that is, $W=(V-b)\1{b\geq 2 V/3}$.
Note how easy it is include the condition on the acceptance by means of the indicator formula.
Now,
\begin{equation*}
\E{W | V=v} = \E((v-b)\1{v \leq 3b/2}} = (v-b)\1{v \leq 3b/2},
\end{equation*}
because everything is a constant.
\end{remark}
And thus,
\begin{align*}
  \E W
  &= \int_{0}^{1} \E{W|V=v} f_{V}(v) \d v \\
  &= \int_{0}^{1} \E{W|V=v}  \d v, \quad \text{$V$ is uniform,} \\
  &= \int_{0}^{1} (v-b)\1{v\leq 3b/2}  \d v \\
  &= \int_{0}^{3b/2} (v-b)  \d v  = -3 b^{2}/8,
\end{align*}
where the last step follows from some simple algebra. This is the same as BH (\smiley ), but I find the reasoning simpler.

\subsection{Background material, not compulsory}
\label{sec:backgr-mater-not}

\begin{exercise}
Example BH.9.1.8 takes a few shortcuts to derive the expectation of the number of tails before heads appears when throwing a coin.
Use conditioning to close the arguments. (This is background material because this is quite hard.)
\begin{solution}
Let $X_{i}$ be the outcome of the $i$th throw of the coin.
If $X_{i}=0$ for $1\leq i< n$ and $X_{n}=1$, we stop.
Thus, the number of failures $T$ is given as $T=\inf\{n\geq 1: X_n=1\} - 1$.
We subtract $1$ because when the first throw results in a $1$ (i.e., a head), we have 0 failures (of the 1 throws).

Here is the first subtlety: we need to take the $\inf$ instead of the $\min$.
The reason is that, formally, we might need to throw an infinite number of times.
But, the minimum of an infinite set is not well defined, for instance $\min\{1/n: n=1, 2, 3, \ldots\}$ does not exist.
To circumvent such problems we can the infimum, because $\inf\{1/n : n = 1, 2, \ldots\} = 0$.

We are interested in $\E T$. With LOTE:
\begin{align*}
  \E{T}
  &= \E{T|X_1=0} \P{X_1=0} + \E{T|X_1=1} \P{X_1=1}.
\end{align*}
Now $\E{T|X_1=1} = 0$ because the first outcome resulted in a 1 so that we can stop before having seen a failure.

How about the other conditional expectation? In this case,
\begin{align*}
  \E{T\mid X_{1} = 0}
  &= \sum_{j=0}^{\infty} j \P{T=j|X_{1} = 0} \\
  &= \sum_{j=0}^{\infty} j \P{T=j, X_{1} = 0}/\P{X_{1} = 0} \\
  &= \frac{1}{q}\sum_{j=0}^{\infty} j \P{T=j, X_{1} = 0}.
\end{align*}
Now  $\{T=0, X_{1} = 0\} = \varnothing$ and  $\{T=k, X_{1} = 0\} = \{T=k\}$ for $k\geq 1$, because $\{T=k\}=\{X_{1}= X_{2} = \cdots =X_{k} = 0, X_{k+1} = 1\}$ for $k\geq 1$.
Therefore,
\begin{align*}
  \E{T\mid X_{1} = 0}
  &= \frac{1}{q}\sum_{j=1}^{\infty} j \P{T=j}
  = \frac{1}{q}\sum_{j=1}^{\infty} j pq^{j} \\
  &= \sum_{j=1}^{\infty} j pq^{j-1}  =
  = \sum_{j=0}^{\infty} (j+1) pq^{j}  = \E{T+1} \\
  &= 1  + \E T.
\end{align*}
Thus, $\E{T|X_{1} = 0} = 1 + \E T$, exactly as BH claims in the example, but now we proved it by elementary means instead of relying on slick (but dangerous) arguments.
\end{solution}
\end{exercise}


\section{Lecture 8}


\subsection{Compulsory material}

\begin{itemize}
\item 9.2: read
\item BH video: 27
\end{itemize}


\begin{exercise}
We draw \emph{with replacement} balls, numbered 1 to $N$, from an urn.
Find a recursion to compute the expected number $\E T$ of draws necessary to see all balls.
\begin{solution}
Write $T_{n}$ for expected time to finish given that we have seen $n$ different balls. Thus, $\E T = T_{0}$.
Then for $n< N$ and using conditioning and LOTE
\begin{align}
T_n
&= 1
+T_{n+1} \P{\text{draw a new ball}}
+T_{n} \P{\text{draw an old ball}}  \\
&= 1
+T_{n+1} \frac{N-n}{N}
+T_{n} \frac{n}{N} \\
&\implies\\
T_n \frac{N-n}{N} &= 1 +T_{n+1} \frac{N-n}{N} \\
&\implies \\
T_n &= \frac{N}{N-n}  +T_{n+1}.
\end{align}
How about the boundary conditions, i.e., what is $T_{n}$ for $n=N$?
That's obvious: $T_{N} = 0$. Hence,
\begin{equation}
T_{N-1} = \frac{N}{N-(N-1)} + T_{N} = N,
\end{equation}
and so on.

If you are interested,  push the result a bit further: approximate $T_{0}$ for $N\gg 0$.
\end{solution}
\end{exercise}


\begin{exercise}
Write code to compute $\E T$ for $N=45$.
\begin{solution}
Here is the python code.

\begin{pyblock}
N = 45


def T(n):
    if n >= N:
        return 0
    return N / (N - n) + T(n + 1)

# print(T(44))
# print(T(0))
\end{pyblock}
\begin{minted}{R}
N = 45

bigT = function(n) {
  if(n >= N) {
    return(0)
  }
  return(N / (N - n) + bigT(n + 1))
}

#print(bigT(44))
#print(bigT(0))
\end{minted}
Here are the results $T(44) = \py{T(44)}$ (check!) and $T(0) = \py{T(0)}$.
\end{solution}
\end{exercise}


\begin{pycode}
from pathlib import Path

exercise_name = "bh-9.25.tex"
fname = Path("../bh_problems") / exercise_name
with fname.open("r") as fp:
    state = 0  # dump
    for line in fp.readlines():
        if line[:16] == r"\begin{exercise}":
            state = 1
        if state == 1:
            print(line.strip())
        if line[:14] == r"\end{exercise}":
            break
\end{pycode}

\subsection{Background material, not compulsory}

\begin{exercise}
We draw, with replacement, balls, numbered 1 to $N$, from an urn, but 6 at a time (not just one as in the previous exercise).
Find a recursion to compute the expected number $\E T$ of draws necessary to see all balls.
\begin{solution}
  Write $T_{n}$ for expected time to finish given that we have seen $n$ different balls.
  Take 6 balls.
  If we would know the number $k$ of new balls drawn, then $T_{n} = 1 + T_{n+k}$.
  What is the probability to draw $k$ new balls  out of the 6 we pick?
  This must be
\begin{equation}
\label{eq:930}
{n \choose 6-k}{N-n \choose k}\big/{N \choose 6}.
\end{equation}
Therefore, by LOTE, and when $N-n\geq 6$
\begin{align}
T_{n} = 1 + \sum_{k=0}^{6} \frac{{n \choose 6-k}{N-n \choose k}}{{N \choose 6}} T_{n+k}.
\end{align}
This formula is not ok when are just 2 new balls as in that case we cannot pick $k=6$ new balls. In general, we can pick $k=\min\{6, N-n\}$ new balls. Hence,
\begin{align}
T_{n}
&=
1 + \sum_{k=0}^{\min\{6, N-n\}} \frac{{n \choose 6-k}{N-n \choose k}}{{N \choose 6}} T_{n+k} \\
&\implies \\
T_{n} - \frac{{n \choose 6}{N-n \choose 0}}{{N \choose 6}} T_{n}
&=1 + \sum_{k=1}^{\min\{6, N-n\}} \frac{{n \choose 6-k}{N-n \choose k}}{{N \choose 6}} T_{n+k} \\
&\implies \\
T_{n}\left( {N\choose 6}  -{n \choose 6} \right)
&={N\choose 6} + \sum_{k=1}^{\min\{6, N-n\}}{n \choose 6-k}{N-n \choose k} T_{n+k} \\
\end{align}
And this is the final result.

\end{solution}
\end{exercise}

\begin{exercise}
For the previous exercise, compute $\E T$ for $N=45$.
\begin{solution}
  When we draw $m=6$ balls at a time, we should take care how we compute the recursion.

  To see this, consider for ease the case in which we draw two balls at at time.  Now I want to know how often each function gets called in the computation. For this, I write a simple helper function $S(n, N)$ that returns $1$ for being called plus $S(n+1, N) + S(n+2, N)$. Like this, I can find out how often the functions are called in the recursion.

\begin{pyblock}
def S(n, N):
    if n >= N:
        return 0
    return 1 + S(n + 1, N) + S(n + 2, N)


for N in range(1, 25):
    print(S(0, N))
\end{pyblock}
\begin{minted}{R}
S = function(n, N) {
  if(n >= N) {
    return(0)
  }
  return(1 + S(n + 1, N) + S(n + 2, N))
}

for (N in 1:24) {
  print(S(0, N))
}
\end{minted}
The result is this: \printpythontex.
Apparently, the number of functions called grows very rapidly.
When we want to compute for 6 balls and $N=45$ the situation must be much, much worse.

The way out is to \emph{store} intermediate results rather than computing them time and again.
For this we use the concept \emph{memoization}.
You should memorize this concept when dealing with the computation of recursions.
In short, this means that we check whether we computed the value of a function earlier.
If so, get the value from memory, otherwise, do the computation, and store it for later purposes.

Let's check what happens if we print out how often the function gets called when using memoization.
\begin{pyblock}
from functools import lru_cache

@lru_cache   # This realizes the memoization.
def S(n, N):
    if n >= N:
        return 0
    print(f"Called for n = {n}\n")
    return 1 + S(n + 1, N) + S(n + 2, N)


S(0, 5)
\end{pyblock}
\begin{minted}{R}
library("memoise")

S = function(n, N) {
  if(n >= N) {
    return(0)
  }
  cat("Called for n =", n, "\n")
  return(1 + S(n + 1, N) + S(n + 2, N))
}
S = memoise(S)

S(0, 5)
\end{minted}
Now the result is this:
\begin{center}
\printpythontex
\end{center}
This is much better, for each $n$ the function in the recursion gets called just once.

To convince you that memoization really works, let's remove the memoization.
\begin{pyblock}
def S(n, N):
    if n >= N:
        return 0
    print(f"Called for n = {n}\n")
    return 1 + S(n + 1, N) + S(n + 2, N)


S(0, 5)
\end{pyblock}
\begin{minted}{R}
S = function(n, N) {
  if(n >= N) {
    return(0)
  }
  cat("Called for n =", n, "\n")
  return(1 + S(n + 1, N) + S(n + 2, N))
}

S(0, 5)
\end{minted}
This is the result:
\begin{center}
\printpythontex
\end{center}
You see how often the function is called with $n=4$?



For the course you don't have to look up on the web how memoization is implemented, but I advice you to read about.
Understanding such ideas makes you much better at programming, which is important in view of the fact that many of you will have to use computers a lot in your profession careers.
Besides this, memoization lies at the heart of how spreadsheet programs such as excel work.

Now we know that we have to use memoization, we can turn to the balls and urn problem.
I build a general function $T(n,m)$ for the situation in which we have seen $n$ balls and we pick $m$ balls at a time.
Like this, I can check with the earlier case in which we picked just one ball by computing $T(n, 1)$.
(Hopefully you learn from this that you should also test your code.)


%\begin{pyblock}[][numbers=left]
\begin{pyblock}
from math import comb
from functools import lru_cache

N = 45


@lru_cache
def T(n, m):
    if n >= N:
        return 0
    res = comb(N, m)
    for k in range(1, min(m, N - n) + 1):
        P = comb(n, m - k)
        P *= comb(N - n, k)
        res += P * T(n + k, m)
    return res / (comb(N, m) - comb(n, m))

\end{pyblock}
\begin{minted}{R}
library("memoise")

N = 45

bigT = function(n, m) {
  if(n >= N) {
    return(0)
  }
  res = choose(N, m)
  for (k in 1:min(m, N - n)) {
    P = choose(n, m - k)
    P = P * choose(N - n, k)
    res = res + P * bigT(n + k, m)
  }
  return(res / (choose(N, m) - choose(n, m)))
}
bigT = memoise(bigT)

bigT(0, 1)
bigT(0, 6)
\end{minted}
Here is the check: $T(0, 1) = \py{T(0, 1)}$, which is the same as our earlier computation.

Next, $T(0, 6) = \py{T(0,6)}$.
This is more than $6$ as fast, i.e., $6\times T(0, 6) = \py{6*T(0, 6)} < T(0,1)$.
Why is that?
Well, the 6 balls we pick from the urn are guaranteed to be drawn without replacement.


Can you solve the following extension? Suppose for each time you draw a set of balls, you get the value (in Euro's say) of the numbers on the balls, and then you put the balls back in the urn.
The game stops when you have seen all balls at least once. What is your expected gain?

\end{solution}
\end{exercise}



\section{Tutorial}
\label{sec:tutorial}


\subsection{Practice}

\begin{exercise}
Let $X$ be a continuous random variable with a pdf
\begin{align}
    f_X(x) = \begin{cases}
    c, &\text{if } 0 \leq x \leq 4, \\
    0, &\text{otherwise}.
    \end{cases}
\end{align}
\begin{enumerate}
    \item What is the value of $c$?
    \item What is the distribution of $X$?
    \item Do we need to know the value of $c$ to determine the distribution of $X$?
\end{enumerate}

\begin{solution}
We need that the pdf $f_X$ integrates to one. Hence, we need
\begin{align}
    \int_{-\infty}^\infty f_X(x) dx &= 1 \qquad \iff \\
    \int_{0}^4 c dx &= 1 \qquad \iff \\
    4c &= 1 \qquad \iff \\
    c &= 1/4.
\end{align}
Clearly, $X$ is uniformly distributed on $[0,4]$. In fact, we do not need to know the value of $c$ to determine this. It is sufficient to know that the pdf of $X$ is constant on the interval $[0,4]$.
\end{solution}
\end{exercise}

\begin{exercise}
Let $X$ be a continuous random variable with a pdf
\begin{align}
    f_X(x) = c \cdot e^{-\frac{(x - 4)^2}{8}}, \quad x \in \R.
\end{align}
\begin{enumerate}
    \item What is the value of $c$?
    \item What is the distribution of $X$?
    \item Do we need to know the value of $c$ to determine the distribution of $X$?
\end{enumerate}
\begin{solution}
We need that the pdf $f_X$ integrates to one. Hence, we need
\begin{align}
    \int_{-\infty}^\infty f_X(x) dx &= 1 \qquad \iff \\
    \int_{-\infty}^\infty c \cdot e^{-\frac{(x - 4)^2}{8}} dx &= 1 \qquad \iff \\
    c \cdot \sqrt{2\pi}\cdot 2 \int_{-\infty}^\infty  \frac{1}{\sqrt{2\pi}\cdot 2} e^{-\frac{1}{2}\frac{(x - 4)^2}{2^2}} dx &= 1 \qquad \iff \\
    c \cdot \sqrt{2\pi}\cdot 2 \cdot 1 &= 1 \qquad \iff \\
    c &= \frac{1}{\sqrt{2\pi}\cdot 2}.
\end{align}
Clearly, $X$ is $N(4,4)$ distributed. In fact, we do not need to know the value of $c$ to determine this. It is sufficient to observe the structure of the pdf as a function of $x$. %Arpan: I corrected the distribution of N() to refer to N(mean,variace) to be consistent with the book.
\end{solution}
\end{exercise}


\subsection{BH Exercises}

\section{Homework}

\label{sec:homework}

\subsection{Practice}
\label{sec:practice}


\begin{exercise}
Let $X,Y$ be i.i.d.
$\mathcal{N}(0,1)$ distributed and define $Z = X + Y$.
Show that $Z \sim \mathcal{N}(0,2)$ using a convolution integral.
\begin{solution}
Recall the pdf of a $\mathcal{N}(\mu,\sigma^2)$ random variable $T$:
\begin{align}
    f_T(t) = \frac{1}{\sqrt{2\pi}\sigma} \exp\left\{-\frac{(t-\mu)^2}{2 \sigma^2}\right\}, \quad t \in \R.
\end{align}
We use a convolution integral. We have for every $z \in \R$:
\begin{align}
    f_Z(z) &= \int_{-\infty}^\infty f_Y(z - x) f_X(x) dx \\
    &= \int_{-\infty}^\infty \frac{1}{\sqrt{2\pi}} \exp\left\{-\frac{(z - x)^2}{2}\right\} \frac{1}{\sqrt{2\pi}} \exp\left\{-\frac{x^2}{2 }\right\} dx \\
    &= \frac{1}{2\pi} \int_{-\infty}^\infty  \exp\left\{-\frac{z^2 - 2zx + 2 x^2}{2}\right\} dx \\
    &= \frac{1}{2\pi} \int_{-\infty}^\infty  \exp\left\{-\left[\frac{1}{2}z^2 - zx +  x^2\right]\right\} dx \\
    &= \frac{1}{2\pi} \int_{-\infty}^\infty  \exp\left\{-\left[\left(x - \frac{1}{2}z\right)^2 + \frac{1}{4}z^2\right]\right\} dx \\
    &= \frac{1}{2\pi} \exp\left\{-\frac{1}{4}z^2\right\} \int_{-\infty}^\infty  \exp\left\{-\left(x - \frac{1}{2}z\right)^2\right\} dx \\
    &= \frac{1}{\sqrt{2\pi} \cdot \sqrt{2}} \exp\left\{-\frac{z^2}{2 \cdot \sqrt{2}^2}\right\} \cdot \int_{-\infty}^\infty  \frac{1}{\sqrt{2\pi} \cdot (1/\sqrt{2})} \exp\left\{-\frac{\left(x - \tfrac{1}{2}z\right)^2}{2 \cdot (1/\sqrt{2})^2}\right\} dx \\
    &= \frac{1}{\sqrt{2\pi} \cdot \sqrt{2}} \exp\left\{-\frac{z^2}{2 \cdot \sqrt{2}^2}\right\},
\end{align}
where in the last step we recognize that the integral on the right is the integral of the pdf of a $\mathcal{N}(\tfrac{1}{2}z, 1/2)$ random variable, which integrates to one (since any pdf integrates to one). We are left with the pdf of a $\mathcal{N}(0,2)$ random variable. Hence, $Z \sim \mathcal{N}(0,2)$.
\end{solution}
\end{exercise}


\subsection{BH Exercises}
\label{sec:bh-exercises-1}

\begin{pycode}
from pathlib import Path

exercise_name = "bh-9.28.tex"
fname = Path("../bh_problems") / exercise_name
with fname.open("r") as fp:
    state = 0  # dump
    for line in fp.readlines():
        if line[:16] == r"\begin{exercise}":
            state = 1
        if state == 1:
            print(line.strip())
        if line[:14] == r"\end{exercise}":
            break
\end{pycode}



% \section{Poleverywhere questions}
% \label{sec:polev-quest}


% TBD


\section{Assignment}
\label{sec:assignment}

For the handwritten solution, take the following exercise.

\begin{pycode}
from pathlib import Path

exercise_name = "bh-8.13.tex"
fname = Path("../bh_problems") / exercise_name
with fname.open("r") as fp:
    state = 0  # dump
    for line in fp.readlines():
        if line[:16] == r"\begin{exercise}":
            state = 1
        if state == 1:
            print(line.strip())
        if line[:14] == r"\end{exercise}":
            break
\end{pycode}


\subfile{../assignments/bh-8-18.tex}



\section{True false questions}
\label{sec:true-false-questions}
\setcounter{theorem}{0}
\subfile{../truefalse/tf-week-4-source-questions.tex}

\opt{all-solutions-at-end}{
\Closesolutionfile{hint}
\Closesolutionfile{ans}
\clearpage
\section{Hints}
\input{hint}
\clearpage
\section{Solutions}
\input{ans}
}

\end{document}
